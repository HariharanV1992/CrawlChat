# CrawlChat Crawler - Production Ready

A clean, organized web crawler for the CrawlChat application.

## ğŸ“ Project Structure

```
crawlchat-crawler/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ crawler/           # Core crawler modules
â”‚   â”‚   â”œâ”€â”€ advanced_crawler.py
â”‚   â”‚   â”œâ”€â”€ enhanced_scrapingbee_manager.py
â”‚   â”‚   â”œâ”€â”€ file_downloader.py
â”‚   â”‚   â”œâ”€â”€ link_extractor.py
â”‚   â”‚   â”œâ”€â”€ proxy_manager.py
â”‚   â”‚   â”œâ”€â”€ s3_cache_manager.py
â”‚   â”‚   â”œâ”€â”€ settings_manager.py
â”‚   â”‚   â”œâ”€â”€ smart_scrapingbee_manager.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â””â”€â”€ lambda_handler.py  # AWS Lambda handler
â”œâ”€â”€ config/                # Configuration files
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ CRAWLER_DOCUMENTATION.md
â”‚   â””â”€â”€ SCRAPINGBEE_PARAMETERS.md
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile            # Docker configuration
â””â”€â”€ README.md            # This file
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- AWS Lambda environment
- ScrapingBee API key

### Installation
```bash
pip install -r requirements.txt
```

### Environment Variables
```bash
SCRAPINGBEE_API_KEY=your_api_key_here
AWS_REGION=ap-south-1
```

## ğŸ”§ Usage

### Lambda Function
The crawler is designed to run as an AWS Lambda function. The main entry point is `src/lambda_handler.py`.

### Local Development
```python
from src.crawler.advanced_crawler import AdvancedCrawler

# Initialize crawler
crawler = AdvancedCrawler(api_key="your_scrapingbee_api_key")

# Crawl a URL
result = crawler.crawl_url("https://example.com")
print(result)
```

## ğŸ“š Documentation

- [Crawler Documentation](docs/CRAWLER_DOCUMENTATION.md)
- [ScrapingBee Parameters](docs/SCRAPINGBEE_PARAMETERS.md)

## ğŸ—ï¸ Architecture

### Core Components
1. **AdvancedCrawler**: Main crawler class with progressive proxy strategy
2. **EnhancedScrapingBeeManager**: Handles ScrapingBee API interactions
3. **ProxyManager**: Manages different proxy modes (premium, stealth, own)
4. **FileDownloader**: Handles file downloads from URLs
5. **LinkExtractor**: Extracts links from crawled content

### Features
- âœ… Progressive proxy strategy (premium â†’ stealth â†’ own)
- âœ… JavaScript rendering support
- âœ… File download capabilities
- âœ… Screenshot functionality
- âœ… S3 caching
- âœ… Error handling and retry logic
- âœ… Usage statistics and cost tracking

## ğŸ”„ Deployment

### Docker
```bash
docker build -t crawlchat-crawler .
docker run -e SCRAPINGBEE_API_KEY=your_key crawlchat-crawler
```

### AWS Lambda
1. Package the `src/` directory
2. Upload to AWS Lambda
3. Set environment variables
4. Configure triggers (API Gateway, SQS, etc.)

## ğŸ§ª Testing

The crawler includes comprehensive error handling and logging. For testing specific scenarios, refer to the documentation files.

## ğŸ“ License

This project is part of the CrawlChat application. 